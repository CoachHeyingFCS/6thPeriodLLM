from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate  
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.retrieval import create_retrieval_chain
import sys
import os

#--Important Variables--#
PDF_PATH = "/workspaces/6thPeriodLLM/Understanding Modelfile in Ollama.pdf"
DB_DIR = "./sql_chroma_db"
MODEL_NAME = "test-llm"

def loadPDF():
    # Pull PDF into code
    loader = PyPDFLoader(PDF_PATH)
    pages = loader.load_and_split()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 700,
        chunk_overlap = 200,
        length_function = len,
        add_start_index = True
    )
    chunks = text_splitter.split_documents(pages)
    print(f"Split {len(pages)} pages into {len(chunks)} chunks")

    embedding = FastEmbedEmbeddings()
    Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=DB_DIR)
    return embedding

def build_or_load_vectors() -> Chroma:
    thisEmbedding = loadPDF()
    if(os.path.exists(DB_DIR) and os.listdir(DB_DIR)):
        print("Loading existing vector store...")
        vector_store = Chroma(
            persist_directory=DB_DIR,
            embedding_function= thisEmbedding,
        )
    else:
        vector_store = loadPDF()
    
    return vector_store

def rag_chain():
    vectors = build_or_load_vectors()

    retriever = vectors.as_retriever(
        search_type = "similarity_score_threshold",
        search_kwargs={
            "k": 2,
            "score_threshold": 0.5,
        }
    )
    model = ChatOllama(model= MODEL_NAME)
    prompt = PromptTemplate.from_template(
        """ 
        You are an AI assistant. Use ONLY the provided context to answer the question. 
        If the answer is not clearly and directly supported by the context, respond exactly with:
        "I don't have enough context to answer that."

        Do NOT make up facts or speculate.
        "You must base your answer ONLY on the provided context. 
        If you include any information from the context, you must reference the filename or page it came from. 
        If there is no relevant context, respond: 'I don't have enough context to answer that.'"

        Context:
        {context}

        Question:
        {input}

        Answer:
        """
    )

    document_chain = create_stuff_documents_chain(model, prompt)
    chain = create_retrieval_chain(retriever, document_chain)
    return chain



def ask(query: str):
    #
    chain = rag_chain()
    # invoke chain
    result = chain.invoke({"input": query})
    # print results
    print(result["answer"])


user_input = input("What is your question?\n\n")
while user_input.lower() != 'exit':
    ask(user_input)
    user_input = input("What is your question?\n\n")
